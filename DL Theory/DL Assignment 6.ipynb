{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8",
      "metadata": {
        "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8"
      },
      "source": [
        "# Assignment 6 Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ca5ec7-2617-47e1-a3af-73f92fe37038",
      "metadata": {
        "id": "b9ca5ec7-2617-47e1-a3af-73f92fe37038"
      },
      "source": [
        "**SUBMITTED BY: MIHIR KUDALE**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00ec14db-ff40-44a7-95d2-8304cbb05ef8",
      "metadata": {
        "id": "00ec14db-ff40-44a7-95d2-8304cbb05ef8"
      },
      "source": [
        "**1. What are the advantages of a CNN over a fully connected DNN for image classification?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "593c14d3-bedd-4c8d-b487-b786c92fa246",
      "metadata": {
        "id": "593c14d3-bedd-4c8d-b487-b786c92fa246"
      },
      "source": [
        "**Ans:** Convolutional Neural Networks (CNNs) have several advantages over fully connected Deep Neural Networks (DNNs) for image classification. Here are some of the main advantages:\n",
        "\n",
        "1. Parameter efficiency: CNNs use shared weights and biases for the convolutional layers, which greatly reduces the number of parameters compared to fully connected DNNs. This makes CNNs more efficient for processing large images and reduces the risk of overfitting.\n",
        "\n",
        "2. Translation invariance: CNNs are designed to recognize patterns and features within an image regardless of their location, making them translation invariant. This is achieved through the use of convolutional layers that slide filters over the image to detect patterns in a local area. This property makes CNNs well-suited for image classification tasks where the object of interest can be anywhere in the image.\n",
        "\n",
        "3. Hierarchical feature learning: CNNs use multiple layers of feature extraction to learn hierarchical representations of images. This allows the network to learn high-level concepts, such as object categories, from low-level features, such as edges and corners. In contrast, fully connected DNNs treat all input features as independent and do not capture the spatial relationships between them.\n",
        "\n",
        "4. Robustness to input variations: CNNs are designed to be robust to variations in the input, such as changes in lighting, rotation, and scale. This is achieved through the use of techniques such as pooling, which reduces the sensitivity of the network to small changes in the input.\n",
        "\n",
        "Overall, the advantages of CNNs over fully connected DNNs make them the state-of-the-art approach for image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8706546-a9c0-470d-a8e2-24799ca9f469",
      "metadata": {
        "id": "d8706546-a9c0-470d-a8e2-24799ca9f469"
      },
      "source": [
        "**2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when\n",
        "training on a mini-batch of 50 images?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69d5959-1094-42b8-990f-2431d361de3f",
      "metadata": {
        "id": "d69d5959-1094-42b8-990f-2431d361de3f"
      },
      "source": [
        "**Ans:** To calculate the total number of parameters in the CNN, we need to count the number of parameters in each layer and then sum them up. The number of parameters in a convolutional layer depends on the number of filters, the size of the filters, and the number of input channels. The formula for calculating the number of parameters in a convolutional layer is:\n",
        "\n",
        "number of parameters = (size of filter * number of input channels + 1) * number of filters\n",
        "\n",
        "Using the given information, we can calculate the number of parameters in each convolutional layer:\n",
        "\n",
        "* The first convolutional layer has 3x3x3x100 + 100 = 2,800 parameters\n",
        "* The second convolutional layer has 3x3x100x200 + 200 = 180,200 parameters\n",
        "* The third convolutional layer has 3x3x200x400 + 400 = 1,160,400 parameters\n",
        "\n",
        "Therefore, the total number of parameters in the CNN is 2,800 + 180,200 + 1,160,400 = 1,343,400 parameters.\n",
        "\n",
        "To calculate the amount of RAM required for prediction or training, we need to consider the size of the input and output tensors and the data type being used. Assuming we are using 32-bit floats (4 bytes), the size of the input tensor for a single image is 200x300x3 = 180,000 bytes. The size of the output tensor for a single image is (200/8)x(300/8)x400x4 = 6,000,000 bytes. Therefore, the total RAM required for prediction for a single instance is approximately 6,180,000 bytes (6.18 MB).\n",
        "\n",
        "If we are training on a mini-batch of 50 images, the total RAM required would be 50 times the RAM required for a single instance, which is approximately 309 MB. However, this is just the memory required for storing the input and output tensors during one forward pass of the network. The actual amount of memory required for training will depend on the batch size, the size of the model, the optimizer being used, and other factors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98d3e4a-d16e-422f-b655-e1fa93a3aafa",
      "metadata": {
        "id": "b98d3e4a-d16e-422f-b655-e1fa93a3aafa"
      },
      "source": [
        "**3. If your GPU runs out of memory while training a CNN, what are five things you could try to\n",
        "solve the problem?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a879e28-5330-42ed-a391-df1ec179ef89",
      "metadata": {
        "id": "9a879e28-5330-42ed-a391-df1ec179ef89"
      },
      "source": [
        "**Ans:** If your GPU runs out of memory while training a CNN, here are five things you could try to solve the problem:\n",
        "\n",
        "1. Decrease batch size: One of the most effective ways to reduce memory usage is to decrease the batch size. This will reduce the number of samples being processed simultaneously, which in turn reduces the amount of memory required.\n",
        "\n",
        "2. Reduce input size: If the input images are very large, downsampling them to a smaller size can also reduce the memory requirements. However, this may come at the cost of reduced performance or accuracy.\n",
        "\n",
        "3. Use mixed precision training: Using mixed precision training, where the weights and activations are stored using 16-bit floating point numbers instead of 32-bit, can significantly reduce memory usage without sacrificing accuracy.\n",
        "\n",
        "4. Reduce model complexity: Another way to reduce memory usage is to reduce the complexity of the model. This can be done by reducing the number of layers, filters, or neurons in the network, or by using a smaller kernel size.\n",
        "\n",
        "5. Use gradient checkpointing: Gradient checkpointing is a technique that trades off computation time for memory usage. Instead of storing all the intermediate activations during the forward pass for use during the backward pass, only a subset of the activations are stored. This reduces memory usage, but increases computation time during the backward pass.\n",
        "\n",
        "These are just some of the techniques that can be used to reduce memory usage during CNN training. The best approach will depend on the specific model, dataset, and hardware being used, and may require experimentation to find the optimal solution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782028c5-6588-4c3d-a3e8-2e38e2e58930",
      "metadata": {
        "id": "782028c5-6588-4c3d-a3e8-2e38e2e58930"
      },
      "source": [
        "**4. Why would you want to add a max pooling layer rather than a convolutional layer with the\n",
        "same stride?**\n",
        "\n",
        "**Ans:** Max pooling layers and convolutional layers with the same stride both reduce the spatial resolution of the feature maps. However, there are some reasons why one might want to use max pooling instead of convolution:\n",
        "\n",
        "1. Non-linearity: Max pooling introduces a non-linear activation function that can help the network learn more complex features. In contrast, a convolutional layer with the same stride only applies a linear transformation to the input.\n",
        "\n",
        "2. Translation invariance: Max pooling is more translation invariant than convolution with stride. This means that the features learned by the network are less sensitive to small shifts in the input image, which can improve the robustness of the network to small variations in the data.\n",
        "\n",
        "3. Computational efficiency: Max pooling is computationally more efficient than convolution with the same stride because it does not require any learnable parameters.\n",
        "\n",
        "4. Reduce overfitting: Max pooling can be used to reduce overfitting by reducing the spatial resolution of the feature maps and introducing some degree of spatial invariance.\n",
        "\n",
        "Overall, max pooling can be a useful tool for reducing the spatial resolution of feature maps while introducing non-linearity, translation invariance, and computational efficiency. However, the choice between max pooling and convolution with the same stride will depend on the specific requirements of the model and the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bd38c00-8f73-40f8-9fa5-def34ae99f97",
      "metadata": {
        "id": "9bd38c00-8f73-40f8-9fa5-def34ae99f97"
      },
      "source": [
        "**5. When would you want to add a local response normalization layer?**\n",
        "\n",
        "**Ans:** A local response normalization (LRN) layer can be added to a CNN to improve its generalization performance by promoting competition among feature maps. The main purpose of LRN is to normalize the response of a neuron by dividing it by the sum of squares of its neighboring activations.\n",
        "\n",
        "Here are some scenarios when one may want to add a local response normalization layer to a CNN:\n",
        "\n",
        "1. Large-scale datasets: LRN is more effective on large-scale datasets as it helps the network generalize better and reduce overfitting. When working with large-scale datasets, adding an LRN layer can improve the performance of the network.\n",
        "\n",
        "2. Convolutional layers with large receptive fields: When the receptive field of a convolutional layer is large, there is a high chance that some feature maps will respond strongly to stimuli that are irrelevant to the target object. In such cases, LRN can help the network suppress the irrelevant feature maps and promote the relevant ones.\n",
        "\n",
        "3. Networks with multiple convolutional layers: When working with CNNs with multiple convolutional layers, adding an LRN layer can help the network learn more robust features by encouraging competition among feature maps. This can help prevent the network from overfitting to the training data.\n",
        "\n",
        "4. Image classification tasks: LRN has been shown to be particularly effective in improving the performance of CNNs on image classification tasks. It can help the network learn more discriminative features, which in turn can improve its classification accuracy.\n",
        "\n",
        "Overall, adding a local response normalization layer to a CNN can be useful in scenarios where there is a need to improve the generalization performance of the network or promote competition among feature maps. However, the decision to add an LRN layer will depend on the specific requirements of the model and the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f3f3afa-fc4f-432c-a61c-a10b290148fc",
      "metadata": {
        "id": "7f3f3afa-fc4f-432c-a61c-a10b290148fc"
      },
      "source": [
        "**6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?**\n",
        "\n",
        "**Ans:** here are the main innovations in each of the listed models:\n",
        "\n",
        "AlexNet:\n",
        "\n",
        "* AlexNet was one of the first successful deep convolutional neural networks for image classification.\n",
        "* It had 5 convolutional layers and 3 fully connected layers.\n",
        "* AlexNet used Rectified Linear Units (ReLU) activation function instead of the traditional sigmoid activation function used in LeNet-5.\n",
        "* It also used data augmentation techniques such as cropping and flipping to reduce overfitting and increase the size of the training dataset.\n",
        "* AlexNet used dropout regularization to further reduce overfitting.\n",
        "* AlexNet was trained on two GPUs simultaneously, which reduced the training time significantly.\n",
        "\n",
        "GoogLeNet:\n",
        "\n",
        "* GoogLeNet introduced the concept of Inception modules, which allowed for the efficient use of multiple filter sizes in a single layer.\n",
        "* It had 22 layers and was the first network to exceed human-level performance on the ImageNet classification task.\n",
        "* GoogLeNet also used global average pooling instead of fully connected layers, which significantly reduced the number of parameters in the network.\n",
        "* It used a 1x1 convolutional layer to reduce the dimensionality of the feature maps before applying more computationally expensive convolutional layers.\n",
        "\n",
        "ResNet:\n",
        "\n",
        "* ResNet introduced the concept of residual blocks, which allowed for the training of very deep networks without suffering from the problem of vanishing gradients.\n",
        "* It had up to 152 layers and achieved state-of-the-art performance on several computer vision tasks.\n",
        "* ResNet used skip connections to connect earlier layers directly to later layers, which helped to propagate gradients more effectively during training.\n",
        "* It also used batch normalization to accelerate training and improve generalization.\n",
        "\n",
        "SENet:\n",
        "\n",
        "* SENet introduced the concept of Squeeze-and-Excitation (SE) blocks, which allowed the network to adaptively recalibrate the importance of each feature map based on the global context of the image.\n",
        "* It achieved state-of-the-art performance on several computer vision tasks.\n",
        "* SENet used a global average pooling layer to reduce the dimensionality of the feature maps before applying the SE blocks.\n",
        "* It also used residual connections to help propagate gradients more effectively during training.\n",
        "\n",
        "Xception:\n",
        "\n",
        "* Xception introduced the concept of depthwise separable convolutions, which separate the spatial filtering and channel-wise filtering operations.\n",
        "* It had fewer parameters and was more computationally efficient than other state-of-the-art models while achieving comparable or better performance.\n",
        "* Xception used residual connections and batch normalization to accelerate training and improve generalization.\n",
        "* It also used global average pooling instead of fully connected layers to reduce the number of parameters in the network.\n",
        "\n",
        "These are just a few of the innovations introduced in each of these models. There have been many other contributions to the field of deep learning for computer vision, each building upon the successes of the previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6857970-24c5-4efe-8137-85b9e837ac46",
      "metadata": {
        "id": "a6857970-24c5-4efe-8137-85b9e837ac46"
      },
      "source": [
        "**7. What is a fully convolutional network? How can you convert a dense layer into a\n",
        "convolutional layer?**\n",
        "\n",
        "**Ans:** A fully convolutional network (FCN) is a neural network architecture that consists entirely of convolutional layers, and no fully connected layers. FCNs are commonly used in image segmentation tasks, where the output is a pixel-wise classification map instead of a single label.\n",
        "\n",
        "To convert a dense layer into a convolutional layer, we need to ensure that the output shape of the convolutional layer is the same as the output shape of the dense layer. We can achieve this by reshaping the weights of the dense layer into the shape of a convolutional kernel.\n",
        "\n",
        "For example, suppose we have a dense layer with 512 units that takes as input a feature map of size 7x7x256, and we want to convert it into a convolutional layer with a 3x3 kernel. We can do this as follows:\n",
        "\n",
        "1. Reshape the weight matrix of the dense layer to have shape (3, 3, 256, 512). This will create a 3x3 convolutional kernel with 256 input channels and 512 output channels.\n",
        "\n",
        "2. Reshape the output of the previous convolutional layer to have shape (batch_size, 7, 7, 256). This is the same shape as the input to the dense layer.\n",
        "\n",
        "3. Apply the convolution operation with the reshaped kernel to the output of the previous convolutional layer. The output of this operation will have shape (batch_size, 5, 5, 512), which is the same as the output of the dense layer.\n",
        "\n",
        "By converting a dense layer into a convolutional layer in this way, we can include it in a fully convolutional network and take advantage of the benefits of convolutional layers, such as weight sharing and translation invariance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d50681-2ac2-42d1-a7bc-63da5dde8835",
      "metadata": {
        "id": "a2d50681-2ac2-42d1-a7bc-63da5dde8835"
      },
      "source": [
        "**8. What is the main technical difficulty of semantic segmentation?**\n",
        "\n",
        "**Ans:** The main technical difficulty of semantic segmentation is to accurately classify each pixel in an image into one of the pre-defined classes (e.g. person, car, tree, sky, etc.). This is a challenging task because it requires the network to learn both the spatial information and the semantic information of the image.\n",
        "\n",
        "Spatial information refers to the location and arrangement of objects in the image, which is important for distinguishing between adjacent objects or objects that overlap. Semantic information refers to the high-level concepts and meaning of the objects in the image, which is important for distinguishing between objects of the same type (e.g. different types of trees).\n",
        "\n",
        "Another difficulty is handling the class imbalance problem, where some classes have much fewer pixels than others. This can lead to a bias towards the majority classes and poor performance on minority classes.\n",
        "\n",
        "To overcome these difficulties, various techniques have been proposed, including the use of encoder-decoder architectures, skip connections, multi-scale features, dilated convolutions, and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "353a3c9c-2fb9-4db7-b0e0-5ae5d3a16d54",
      "metadata": {
        "tags": [],
        "id": "353a3c9c-2fb9-4db7-b0e0-5ae5d3a16d54"
      },
      "source": [
        "**9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.**\n",
        "\n",
        "**Ans:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ad8f72fc-8cbe-4a2f-8884-3523c4a6f167",
      "metadata": {
        "tags": [],
        "id": "ad8f72fc-8cbe-4a2f-8884-3523c4a6f167"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a2e39a3-550a-421d-b514-57056162c22d",
      "metadata": {
        "tags": [],
        "id": "4a2e39a3-550a-421d-b514-57056162c22d"
      },
      "outputs": [],
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fe512564-925c-4806-84b5-d2d648b00db5",
      "metadata": {
        "tags": [],
        "id": "fe512564-925c-4806-84b5-d2d648b00db5"
      },
      "outputs": [],
      "source": [
        "# Normalize the pixel values to the range [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c668134e-e1c5-45c9-87d9-ea8622f82c28",
      "metadata": {
        "tags": [],
        "id": "c668134e-e1c5-45c9-87d9-ea8622f82c28"
      },
      "outputs": [],
      "source": [
        "# Add a channel dimension to the images\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "25cf07a5-48b2-4020-92d3-50cc20f38f7c",
      "metadata": {
        "tags": [],
        "id": "25cf07a5-48b2-4020-92d3-50cc20f38f7c"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "098a50e8-da2d-460d-be51-379c64a33a46",
      "metadata": {
        "tags": [],
        "id": "098a50e8-da2d-460d-be51-379c64a33a46"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1a1e18ae-912b-43f8-91a6-49b77e3709a6",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a1e18ae-912b-43f8-91a6-49b77e3709a6",
        "outputId": "210b4a03-f0e9-4f98-fc37-9fbf9318a0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 15s 6ms/step - loss: 0.1990 - accuracy: 0.9397 - val_loss: 0.0435 - val_accuracy: 0.9851\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0733 - accuracy: 0.9786 - val_loss: 0.0304 - val_accuracy: 0.9891\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0525 - accuracy: 0.9844 - val_loss: 0.0301 - val_accuracy: 0.9910\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0433 - accuracy: 0.9865 - val_loss: 0.0277 - val_accuracy: 0.9905\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.0213 - val_accuracy: 0.9938\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0301 - accuracy: 0.9910 - val_loss: 0.0226 - val_accuracy: 0.9930\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0266 - accuracy: 0.9916 - val_loss: 0.0243 - val_accuracy: 0.9924\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.0218 - val_accuracy: 0.9929\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.0360 - val_accuracy: 0.9904\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0172 - accuracy: 0.9944 - val_loss: 0.0272 - val_accuracy: 0.9925\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9ba64d27-71a8-4e8c-88fa-3f86c2ef0b01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ba64d27-71a8-4e8c-88fa-3f86c2ef0b01",
        "outputId": "d2ad7050-2492-45a8-a515-97caf1519849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0272 - accuracy: 0.9925 - 690ms/epoch - 2ms/step\n",
            "\n",
            "Test accuracy: 0.9925000071525574\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "466c8229-749c-4516-9774-8c05884f78fd",
      "metadata": {
        "id": "466c8229-749c-4516-9774-8c05884f78fd"
      },
      "source": [
        "**10. Use transfer learning for large image classification, going through these steps:**\n",
        "\n",
        "**a. Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
        "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).**\n",
        "\n",
        "**b. Split it into a training set, a validation set, and a test set.**\n",
        "\n",
        "**c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.**\n",
        "\n",
        "**d. Fine-tune a pretrained model on this dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4b9e08-377d-4d01-9bdc-dd85378c8c84",
      "metadata": {
        "id": "7a4b9e08-377d-4d01-9bdc-dd85378c8c84"
      },
      "source": [
        "**Ans:** **a. Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "metadata": {
        "id": "OfO8XhiHBd9c"
      },
      "id": "OfO8XhiHBd9c",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)"
      ],
      "metadata": {
        "id": "iCh8HuvqCfrI"
      },
      "id": "iCh8HuvqCfrI",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_crop(image, size=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(preprocess_image).batch(BATCH_SIZE)\n",
        "test_ds = test_ds.map(preprocess_image).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "SBlG7zBNCtJg"
      },
      "id": "SBlG7zBNCtJg",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3"
      ],
      "metadata": {
        "id": "f5nLzW5BC4H6"
      },
      "id": "f5nLzW5BC4H6",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds, epochs=5, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnATesZMC-_S",
        "outputId": "a5a342ea-4b3b-4ba9-acff-9cbc9174e3af"
      },
      "id": "jnATesZMC-_S",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "582/582 [==============================] - 67s 102ms/step - loss: 0.0500 - accuracy: 0.9823 - val_loss: 0.0392 - val_accuracy: 0.9884\n",
            "Epoch 2/5\n",
            "582/582 [==============================] - 53s 89ms/step - loss: 0.0365 - accuracy: 0.9873 - val_loss: 0.0469 - val_accuracy: 0.9837\n",
            "Epoch 3/5\n",
            "582/582 [==============================] - 58s 97ms/step - loss: 0.0318 - accuracy: 0.9896 - val_loss: 0.0324 - val_accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "582/582 [==============================] - 54s 90ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.0386 - val_accuracy: 0.9875\n",
            "Epoch 5/5\n",
            "582/582 [==============================] - 58s 97ms/step - loss: 0.0244 - accuracy: 0.9922 - val_loss: 0.0419 - val_accuracy: 0.9880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww_lAmXWDIGb",
        "outputId": "5d2d37a6-0768-4542-c00a-c6a87566fbcf"
      },
      "id": "Ww_lAmXWDIGb",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 6s 76ms/step - loss: 0.0284 - accuracy: 0.9927\n",
            "Test accuracy: 0.9926913380622864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2771a6e-3eab-46ab-9ebf-43e74f795355",
      "metadata": {
        "id": "c2771a6e-3eab-46ab-9ebf-43e74f795355"
      },
      "source": [
        "**b. Split it into a training set, a validation set, and a test set.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)\n"
      ],
      "metadata": {
        "id": "jz_2GT2zK3s-"
      },
      "id": "jz_2GT2zK3s-",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ccfb8320-c3b0-4951-b280-fcca503f493f",
      "metadata": {
        "id": "ccfb8320-c3b0-4951-b280-fcca503f493f"
      },
      "source": [
        "**c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "metadata": {
        "id": "GC-ziWJsLds2"
      },
      "id": "GC-ziWJsLds2",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)\n"
      ],
      "metadata": {
        "id": "gE9Ms6hPM4Og"
      },
      "id": "gE9Ms6hPM4Og",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing functions\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    # Resize the image to the input size of the model\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    # Convert the pixel values to the range [0, 1]\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "def augment_image(image, label):\n",
        "    # Randomly flip the image horizontally\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    # Randomly adjust the brightness of the image\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "jd1lc5T3NA7o"
      },
      "id": "jd1lc5T3NA7o",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training dataset\n",
        "train_ds = train_ds.shuffle(10000)\n",
        "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(batch_size=32)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "1IXREergNI7f"
      },
      "id": "1IXREergNI7f",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the validation dataset\n",
        "val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(batch_size=32)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "eLmTJhPNNNyU"
      },
      "id": "eLmTJhPNNNyU",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the test dataset\n",
        "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size=32)\n",
        "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "BAOa0LscNW52"
      },
      "id": "BAOa0LscNW52",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7b309d55-c50f-40b3-be88-42213ff144e7",
      "metadata": {
        "id": "7b309d55-c50f-40b3-be88-42213ff144e7"
      },
      "source": [
        "**d. Fine-tune a pretrained model on this dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.applications import MobileNetV2"
      ],
      "metadata": {
        "id": "AtdHGAk9N4Mq"
      },
      "id": "AtdHGAk9N4Mq",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
        "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "                                             with_info=True,\n",
        "                                             as_supervised=True)"
      ],
      "metadata": {
        "id": "ZZQJsT6hN4tK"
      },
      "id": "ZZQJsT6hN4tK",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing functions\n",
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "def preprocess_image(image, label):\n",
        "    # Resize the image to the input size of the model\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "    # Convert the pixel values to the range [-1, 1]\n",
        "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "e1W04Mw_PRL3"
      },
      "id": "e1W04Mw_PRL3",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the datasets\n",
        "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_image).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.map(preprocess_image).batch(32)"
      ],
      "metadata": {
        "id": "Ia8-GBa6OjUm"
      },
      "id": "Ia8-GBa6OjUm",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained MobileNetV2 model\n",
        "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "id": "dgt_oJ9wPhM3"
      },
      "id": "dgt_oJ9wPhM3",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the base model's layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "Ijlz1bjzPnGY"
      },
      "id": "Ijlz1bjzPnGY",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new classification layer on top of the base model\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(global_average_layer)"
      ],
      "metadata": {
        "id": "jg311FPLPqfW"
      },
      "id": "jg311FPLPqfW",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the fine-tuned model\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=output_layer)"
      ],
      "metadata": {
        "id": "sp9t-USlPwFy"
      },
      "id": "sp9t-USlPwFy",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "6YInkfxGP0yb"
      },
      "id": "6YInkfxGP0yb",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training set\n",
        "history = model.fit(train_ds, epochs=10, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "AkpieKqGP4qe",
        "outputId": "95a05d19-b414-4195-a878-78a43ededb54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AkpieKqGP4qe",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "582/582 [==============================] - 46s 72ms/step - loss: 0.0595 - accuracy: 0.9797 - val_loss: 0.0392 - val_accuracy: 0.9850\n",
            "Epoch 2/10\n",
            "582/582 [==============================] - 41s 69ms/step - loss: 0.0296 - accuracy: 0.9895 - val_loss: 0.0362 - val_accuracy: 0.9875\n",
            "Epoch 3/10\n",
            "582/582 [==============================] - 41s 68ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 0.0369 - val_accuracy: 0.9880\n",
            "Epoch 4/10\n",
            "582/582 [==============================] - 41s 69ms/step - loss: 0.0207 - accuracy: 0.9929 - val_loss: 0.0393 - val_accuracy: 0.9884\n",
            "Epoch 5/10\n",
            "582/582 [==============================] - 41s 68ms/step - loss: 0.0184 - accuracy: 0.9938 - val_loss: 0.0388 - val_accuracy: 0.9884\n",
            "Epoch 6/10\n",
            "582/582 [==============================] - 41s 68ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.0390 - val_accuracy: 0.9871\n",
            "Epoch 7/10\n",
            "582/582 [==============================] - 41s 68ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.0423 - val_accuracy: 0.9862\n",
            "Epoch 8/10\n",
            "582/582 [==============================] - 40s 67ms/step - loss: 0.0147 - accuracy: 0.9955 - val_loss: 0.0424 - val_accuracy: 0.9880\n",
            "Epoch 9/10\n",
            "582/582 [==============================] - 41s 68ms/step - loss: 0.0139 - accuracy: 0.9954 - val_loss: 0.0440 - val_accuracy: 0.9871\n",
            "Epoch 10/10\n",
            "582/582 [==============================] - 42s 70ms/step - loss: 0.0120 - accuracy: 0.9961 - val_loss: 0.0446 - val_accuracy: 0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "ArlvpyZjP8Ta",
        "outputId": "85114609-c17d-4e85-f2d8-ff49454d3da0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ArlvpyZjP8Ta",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73/73 [==============================] - 5s 69ms/step - loss: 0.0271 - accuracy: 0.9901\n",
            "Test loss: 0.02713381126523018, Test accuracy: 0.990111768245697\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}