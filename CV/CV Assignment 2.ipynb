{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8",
   "metadata": {},
   "source": [
    "# Assignment 2 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5ec7-2617-47e1-a3af-73f92fe37038",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55975a-210a-4ac8-a62d-c8e21d76c575",
   "metadata": {},
   "source": [
    "**1. Explain convolutional neural network, and how does it work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1740fe1-b065-4f2b-a8f1-2ff0ee262f0d",
   "metadata": {},
   "source": [
    "**Ans:** A convolutional neural network (CNN) is a type of artificial neural network used in image recognition and processing that is specifically designed to process pixel data. CNNs use a combination of convolutional layers, pooling layers, and fully connected layers to identify patterns in the data. Convolutional layers use a filter to scan across the image and detect specific features, such as lines, corners, and edges. Pooling layers reduce the dimensionality of the image by combining pixels together. The fully connected layers then combine these features to identify objects and classify images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1763c-6236-49a6-a404-95c8fe0906d7",
   "metadata": {},
   "source": [
    "**2. How does refactoring parts of your neural network definition favor you?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece118e4-5b19-413f-8b23-c1af1c56c7fd",
   "metadata": {},
   "source": [
    "**Ans:** Refactoring parts of your neural network definition can make the code easier to understand, debug, and maintain. It also allows you to reuse code, making it easier to experiment with different model architectures. By making the code more modular, it is also easier to add or remove layers, adjust hyperparameters, or even extend the network with additional features. Refactoring can also help make your code more efficient, as it allows you to reuse code and optimize calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9b8c6-4fd4-4be5-bdb5-1a78622dcf01",
   "metadata": {},
   "source": [
    "**3. What does it mean to flatten? Is it necessary to include it in the MNIST CNN? What is the reason\n",
    "for this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceca902-5cf7-4808-a888-a47b4cdc5a2d",
   "metadata": {},
   "source": [
    "**Ans:** To flatten is to transform a multi-dimensional array into a single dimensional array. It is necessary to include it in the MNIST CNN to transform the output of the convolutional layers into a single dimensional array before the dense layer. This is because the dense layer requires a one-dimensional array as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0771d16-9817-4890-a45c-68f323b50529",
   "metadata": {},
   "source": [
    "**4. What exactly does NCHW stand for?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe56af17-d43c-4d54-a82a-1cc1d5a83c1a",
   "metadata": {},
   "source": [
    "**Ans:** NCHW stands for \"Nested Channel-wise High-dimensional Weighting\" and is a type of deep learning architecture. It is used to improve the accuracy of predictive models by utilizing a multi-level hierarchical structure to better understand complex data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ad2b3-7519-4ff8-afd9-dbee57b7298d",
   "metadata": {},
   "source": [
    "**5. Why are there 7*7*(1168-16) multiplications in the MNIST CNN&#39;s third layer?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ad89fb-6a1f-4fdf-b4e9-3cdb54919feb",
   "metadata": {},
   "source": [
    "**Ans:** The 7x7x(1168-16) multiplications in the MNIST CNN's third layer is the result of the number of parameters in the layer. The first two layers of the MNIST CNN have 1168 input neurons and 16 convolutional filters. Each of these filters has a 7x7 parameter matrix, resulting in 1168 x 16 = 7 x 7 x (1168 - 16) multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09358b-08b0-4f8a-8418-bd46b228376d",
   "metadata": {},
   "source": [
    "**6. Explain definition of receptive field?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c64e9d-aed1-40e3-8432-1318801e529a",
   "metadata": {},
   "source": [
    "**Ans:** A receptive field is a specific region in the visual field of an individual (or a neuron) that responds most strongly to a particular stimulus. It is the area within which a stimulus produces a detectable response in the neuron or individual, and is usually defined by the size and shape of the stimulus that produces the greatest response. The receptive field is often used to analyze the visual processing of the brain, as well as to understand the physiology of the visual system. Receptive fields can also be used to study the effects of visual stimuli on behavior, such as how a person responds to an object in their visual field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e873fd95-b10c-43c0-96e4-c6d19dec4972",
   "metadata": {},
   "source": [
    "**7. What is the scale of an activation&#39;s receptive field after two stride-2 convolutions? What is the\n",
    "reason for this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf5342-5a57-4fac-82cc-000cd6f4f593",
   "metadata": {},
   "source": [
    "**Ans:** After two stride-2 convolutions, the scale of the receptive field is 4. This is because stride-2 convolutions reduce the size of the receptive field by half each time, so two stride-2 convolutions reduces the size of the receptive field by a factor of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0e8a6-b751-4fff-b015-c581d4f24277",
   "metadata": {},
   "source": [
    "**8. What is the tensor representation of a color image?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56acfb01-23df-4362-b435-141fc106ab30",
   "metadata": {},
   "source": [
    "**Ans:** A tensor representation of a color image is a three-dimensional array that contains the values of each pixel in the image. The first two dimensions represent the height and width of the image, while the third dimension represents the color channels (e.g. Red, Green, Blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a579b-ceab-41d5-81c1-88874e859012",
   "metadata": {},
   "source": [
    "**9. How does a color input interact with a convolution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b4fc1-527d-4723-bffe-2d44cd3c98a9",
   "metadata": {},
   "source": [
    "**Ans:** A color input can interact with a convolution by using a color filter. The convolution will apply the filter to the input image, transforming the input into a new image with the same dimensions as the original but with the colors altered according to the filter. The convolution will essentially process the input image and create a new image with the desired color effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe95f4c0-1250-47a6-8d2e-843b5f634b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
