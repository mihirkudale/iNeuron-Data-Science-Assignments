{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8",
   "metadata": {},
   "source": [
    "# Assignment 7 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5ec7-2617-47e1-a3af-73f92fe37038",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35388f-75d7-4485-ac39-15d85c13ffcd",
   "metadata": {},
   "source": [
    "**1. What is the COVARIATE SHIFT Issue, and how does it affect you?**\n",
    "\n",
    "**Ans:** Covariate shift is an issue arising when training and test data are drawn from different distributions. It occurs when the distributions of the input variables in the training and test data are different or when the joint distributions of the input and output variables in the training and test data are different. This can lead to poor performance when a model trained on the training data is applied to the test data. It affects you by reducing the accuracy of your model when it is applied to data outside the training set. This can be especially problematic when working with high-dimensional data, as it is often more difficult to detect the differences between training and test data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749dab4-d4a5-4385-acd2-889e03df9c3b",
   "metadata": {},
   "source": [
    "**2. What is the process of BATCH NORMALIZATION?**\n",
    "\n",
    "**Ans:** Batch normalization is a technique used to normalize data within a mini-batch by subtracting the mean and dividing by the standard deviation. It is used to reduce the internal covariate shift of the model which can help improve the modelâ€™s accuracy. Batch normalization also helps reduce the amount of computational resources required for training by reducing the number of parameters to be adjusted. The process consists of adding two parameters to the model: a learnable scale and a learnable shift. These parameters are learned during the training process and enable the model to adjust itself to the distribution of the data. Batch normalization can help improve the accuracy and convergence of the model, reduce overfitting, and speed up training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ab0f1-44c5-4a25-a16a-71883996a17e",
   "metadata": {},
   "source": [
    "**3. Using our own terms and diagrams, explain LENET ARCHITECTURE.**\n",
    "\n",
    "**Ans:** A LeNet architecture is a type of deep neural network that consists of multiple convolutional layers, pooling layers, and fully connected layers. It was developed by Yann LeCun in 1989 and is the foundation of modern convolutional neural networks.\n",
    "\n",
    "The architecture of LeNet includes the following layers: \n",
    "\n",
    "1. Convolutional Layer: This layer applies a set of convolution filters to the input image, each filter produces a feature map.\n",
    "\n",
    "2. Pooling Layer: This layer reduces the spatial size of the feature map by performing down sampling.\n",
    "\n",
    "3. Fully Connected Layer: This layer consists of neurons that are fully connected to all the neurons in the previous layer.\n",
    "\n",
    "4. Output Layer: This layer performs the classification task by transforming the output of the fully connected layer into a probability distribution over the classes.\n",
    "\n",
    "![Image of LeNet Architecture](https://miro.medium.com/max/700/1*1TI1aGBZ4dybR6__DI9dzA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5798fa-d5e3-402c-ad5d-9c59634a502f",
   "metadata": {},
   "source": [
    "**4. Using our own terms and diagrams, explain ALEXNET ARCHITECTURE.**\n",
    "\n",
    "**Ans:** AlexNet is a convolutional neural network architecture used in deep learning. It is the first deep learning model to win the ImageNet competition in 2012. \n",
    "\n",
    "The AlexNet architecture consists of five convolutional layers and three fully-connected layers. The convolutional layers apply convolution operations to the input image to extract features at different levels of abstraction. The fully-connected layers are used for classification. \n",
    "\n",
    "![Alexnet Architecture Diagram](https://miro.medium.com/max/960/0*pJ3o_2zTTNnixhKH.png)\n",
    "\n",
    "AlexNet architecture consists of: \n",
    "\n",
    "- **Convolutional Layers**: \n",
    "  - The first convolutional layer has 96 filters with a size of 11x11 and stride of 4. \n",
    "  - The second convolutional layer has 256 filters with a size of 5x5 and stride of 1. \n",
    "  - The third convolutional layer has 384 filters with a size of 3x3 and stride of 1. \n",
    "  - The fourth convolutional layer has 384 filters with a size of 3x3 and stride of 1. \n",
    "  - The fifth convolutional layer has 256 filters with a size of 3x3 and stride of 1. \n",
    "\n",
    "- **Max-Pooling Layers**: \n",
    "  - The first max-pooling layer has a pool size of 3x3 and stride of 2. \n",
    "  - The second max-pooling layer has a pool size of 3x3 and stride of 2. \n",
    "\n",
    "- **Fully-Connected Layers**: \n",
    "  - The first fully-connected layer has 4096 neurons. \n",
    "  - The second fully-connected layer has 4096 neurons. \n",
    "  - The third fully-connected layer has 1000 neurons. \n",
    "\n",
    "- **Dropout Layer**: This layer is used for regularization purposes and prevents overfitting. \n",
    "\n",
    "- **Softmax Layer**: This layer is used for classification and produces a probability distribution for the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3ad20-ba9b-4d07-835d-d5a68dce12c8",
   "metadata": {},
   "source": [
    "**5. Describe the vanishing gradient problem.**\n",
    "\n",
    "**Ans:** The vanishing gradient problem is a phenomenon in which the gradients of a neural network's weights, when backpropagated, become so small that they virtually vanish. This is due to the fact that the gradients are multiplied together in the backpropagation process which causes the gradients to shrink exponentially as they propagate backwards through the layers of the network. As a result, the weights of earlier layers in the network are not updated effectively and the network is not able to learn effectively. This can lead to poor performance and slow training of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d203ded-111c-4015-990a-fc2e5b0b6cfc",
   "metadata": {},
   "source": [
    "**6. What is NORMALIZATION OF LOCAL RESPONSE?**\n",
    "\n",
    "**Ans:** Normalization of local response (NLR) is a technique used to normalize the responses of a convolutional neural network (CNN) across different image regions. This technique helps to make the prediction of a model more robust and accurate. NLR works by normalizing the input data, which helps to reduce the effects of local variation in the input. The normalized responses are then used to make predictions. NLR can be used to improve the accuracy of object recognition, facial recognition, and other computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0714b0-da60-4728-9c80-6559e6e5e282",
   "metadata": {},
   "source": [
    "**7. In AlexNet, what WEIGHT REGULARIZATION was used?**\n",
    "\n",
    "**Ans:** AlexNet used a form of weight regularization known as dropout regularization. This technique randomly sets neurons in the network to zero during training, reducing the number of weights and neurons that are used in the network. This helps reduce overfitting and improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c203e75-10ec-4c29-8a3e-df9c355a58b5",
   "metadata": {},
   "source": [
    "**8. Using our own terms and diagrams, explain VGGNET ARCHITECTURE.**\n",
    "\n",
    "**Ans:** VGGNet (Visual Geometry Group Network) is a popular convolutional neural network architecture that was developed by Oxford's Visual Geometry Group in 2014. VGGNet is a deep convolutional neural network composed of 16 layers, including 13 convolutional layers and 3 fully-connected layers. The architecture consists of alternating convolutional layers with a 3x3 filter size and a 2x2 max pooling layer followed by a fully connected layer with a ReLU (Rectified Linear Unit) activation.\n",
    "\n",
    "![VGGNet Architecture](https://miro.medium.com/max/1024/1*hs8Ud3X2LBzf5XMAFTmGGw.jpeg)\n",
    "\n",
    "The architecture of VGGNet consists of a series of convolutional layers with small filter sizes of 3x3 pixels, followed by a max pooling layer. This allows the network to capture fine-grained features while also reducing the computational complexity. The number of convolutional layers increases as the network goes deeper, and the number of filters increases with each layer. The fully-connected layers at the end of the network combine the features extracted by the convolutional layers to classify the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114d4556-2764-4aa2-8ad4-fa5a5534104f",
   "metadata": {},
   "source": [
    "**9. Describe VGGNET CONFIGURATIONS.**\n",
    "\n",
    "**Ans:** VGGNet was developed by the Visual Geometry Group (VGG) at Oxford, and is a deep convolutional neural network (CNN) comprised of 16 layers. It is one of the most advanced and influential architectures in the field of computer vision. VGGNet configurations are characterized as having only 3x3 convolutional filters and a deep stack of convolutional layers that are followed by a few fully connected layers. It is also known to have a lot of parameters and can be quite slow to train. The main configurations of VGGNet are VGG-16 and VGG-19, the two models that were trained on the ImageNet dataset for the ILSVRC-2014 competition. VGG-16 has 16 layers, 13 convolutional layers and 3 fully connected layers, while VGG-19 has 19 layers, 16 convolutional layers and 3 fully connected layers. Both models use small 3x3 filters, but differ in the number of convolutional layers. VGG-16 uses more layers with smaller strides, while VGG-19 uses fewer layers with larger strides. VGG-16 and VGG-19 both use the same input size, 224 x 224. The output size of VGG-16 is 1000, while the output size of VGG-19 is 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67478226-c3e4-4d18-8227-1a3839ef4dca",
   "metadata": {},
   "source": [
    "**10. What regularization methods are used in VGGNET to prevent overfitting?**\n",
    "\n",
    "**Ans:** Regularization methods used in VGGNet to prevent overfitting include weight decay, dropout, and batch normalization. Weight decay is a form of regularization where a penalty is applied to the weights in a model to reduce their values and help reduce overfitting. Dropout is a technique where nodes in a neural network are randomly ignored during training, which can help reduce overfitting. Batch normalization is a technique where the inputs to each layer in a neural network are normalized, which helps reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24dc22-f186-4f57-b410-baf16ac41042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "94f538a246db0fa2df5e0015ac38ebae58f4075808dc245f921dd0fb55023058"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
