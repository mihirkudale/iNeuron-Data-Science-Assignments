{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de7136-d5b7-4bc7-8f8a-bc75b3b574dd",
   "metadata": {},
   "source": [
    "# Assignment 3 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca395c-acf0-4041-bfe9-480d40c950a7",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ad649-1508-4071-a977-fc62afe0359e",
   "metadata": {},
   "source": [
    "**1. Explain the basic architecture of RNN cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74134358-75d9-44e3-9da3-384f46a26866",
   "metadata": {},
   "source": [
    "**Ans:** RNN cells are a type of neural network architecture commonly used in natural language processing (NLP) tasks.\n",
    "\n",
    "RNN cell is composed of three main parts:\n",
    "\n",
    "1. Input Gate: Responsible for receiving input, processing it and deciding whether to update the hidden state or not.\n",
    "\n",
    "2. Output Gate: Responsible for generating output from the hidden state.\n",
    "\n",
    "3. Hidden State: This is a vector which stores information from the previous time step. It is updated by the Input Gate with new information from the current time step. This hidden state is then used to generate output at the current time step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba7fc37-00a1-43d3-b82d-9f2414f73329",
   "metadata": {},
   "source": [
    "**2. Explain Backpropagation through time (BPTT)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98a1ef-abfa-4c05-bb7a-d058ed0a86bc",
   "metadata": {},
   "source": [
    "**Ans:** Backpropagation Through Time (BPTT) is a type of supervised learning algorithm used to train recurrent neural networks (RNNs). It is an extension of the standard backpropagation algorithm, which is used to train feedforward neural networks. BPTT works by unrolling the RNN over time and then performing backpropagation over the unfolded network. This allows the algorithm to calculate the gradients of the cost function with respect to the weights of the RNN. These gradients can then be used to update the weights in order to minimize the cost function. BPTT is an effective algorithm for training RNNs, but it has the disadvantage of being computationally expensive and requiring a large amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a50a90-918b-4fab-89a9-05b5d73395de",
   "metadata": {},
   "source": [
    "**3. Explain Vanishing and exploding gradients**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf4ab57-f8af-4696-8cd9-25a2a06c7f4b",
   "metadata": {},
   "source": [
    "**Ans:** Vanishing gradients refer to a phenomenon in neural networks where the gradient during backpropagation becomes increasingly smaller and smaller. This usually occurs when the model has many layers and can lead to the model’s weights and biases not updating as expected. This can cause the model to not learn as quickly and can even lead to the model not learning at all.\n",
    "\n",
    "Exploding gradients refer to the opposite phenomenon in neural networks, where the gradient during backpropagation becomes increasingly larger and larger. This can happen when the model has many layers and can lead to the model’s weights and biases updating too quickly. This can cause the model to overfit the training data and can lead to the model not generalizing well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff2077-0a3a-4142-a3de-d8e5c9c030da",
   "metadata": {},
   "source": [
    "**4. Explain Long short-term memory (LSTM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdee1c-f7a1-4282-8b8a-98410aedd71a",
   "metadata": {},
   "source": [
    "**Ans:** Long short-term memory (LSTM) is a type of recurrent neural network (RNN) that is capable of learning long-term dependencies. It is well-suited to tasks such as natural language processing and speech recognition, where long-term context is important. The LSTM architecture consists of memory cells, input gates, output gates, and forget gates, which help the network remember information over a long period of time. The memory cells are responsible for storing the information, while the gates control the flow of information into and out of the cells. LSTM networks are trained using backpropagation through time and have been used to achieve state-of-the-art performance on a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4c92a-42a1-465c-951b-14704d50199d",
   "metadata": {},
   "source": [
    "**5. Explain Gated recurrent unit (GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a49cf0-8b8f-4cbc-a467-0a6913dd2949",
   "metadata": {},
   "source": [
    "**Ans:** Gated Recurrent Unit (GRU) is a type of Recurrent Neural Network (RNN) architecture that is similar to Long Short-Term Memory (LSTM) networks, but uses fewer parameters and has fewer layers. Like LSTM, GRU has a gating mechanism that controls the flow of data within the network. The GRU architecture consists of two gates: a reset gate and an update gate. The reset gate determines how much of the past information to forget, while the update gate decides how much of the new data to use in the current state. The GRU architecture is designed to learn long-term dependencies, allowing it to capture and store more contextual information from the input data, enabling it to make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56028b7-36f0-4b76-b855-181ed25be220",
   "metadata": {},
   "source": [
    "**6. Explain Peephole LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d02cbd-2af3-4bc4-9d7e-3ee579e17a09",
   "metadata": {},
   "source": [
    "**Ans:** Peephole LSTM is a type of Long Short-Term Memory (LSTM) network. It is an extension of the traditional LSTM network that allows for the direct connection between the current cell state and the current input gate. This direct connection allows the network to make use of more contextual information when making decisions about how to process incoming input. Additionally, peephole LSTM networks are able to better capture long-term dependencies in the data. This is particularly useful for tasks such as speech recognition and natural language processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff028b-bb90-4569-8b1b-35cc3b0538ae",
   "metadata": {},
   "source": [
    "**7. Bidirectional RNNs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c89896-0af9-453f-9ad3-5471c0ef4332",
   "metadata": {},
   "source": [
    "**Ans:** Bidirectional RNNs are recurrent neural networks that process data in both forward and backward directions. This allows them to capture and process long-term dependencies in both directions, which is beneficial for tasks such as language modeling and machine translation. Bidirectional RNNs are constructed by having two separate RNNs, one that processes the data in the forward direction and one that processes the data in the backward direction. The outputs of the two RNNs are then merged in some way, typically by concatenating them together. Bidirectional RNNs can significantly improve the performance of machine learning models on tasks such as language modeling and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c763e-eb90-4c83-9557-c48e7bbc8cef",
   "metadata": {},
   "source": [
    "**8. Explain the gates of LSTM with equations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d3615-5541-4abd-8c73-684ad89874aa",
   "metadata": {},
   "source": [
    "**Ans:** The gates of LSTM are used to control the flow of information into and out of the memory cell.\n",
    "\n",
    "The gates are defined by the following equations:\n",
    "\n",
    "Input Gate: \n",
    "i_t = σ(W_i · [h_t-1, x_t] + b_i)\n",
    "\n",
    "Forget Gate: \n",
    "f_t = σ(W_f · [h_t-1, x_t] + b_f)\n",
    "\n",
    "Output Gate: \n",
    "o_t = σ(W_o · [h_t-1, x_t] + b_o)\n",
    "\n",
    "Cell State: \n",
    "c_t = f_t * c_t-1 + i_t * tanh(W_c · [h_t-1, x_t] + b_c)\n",
    "\n",
    "Hidden State: \n",
    "h_t = o_t * tanh(c_t)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425e349-d140-4096-9caa-586841b726bd",
   "metadata": {},
   "source": [
    "**9. Explain BiLSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c32d16c-6a14-463f-886e-244d5444cf76",
   "metadata": {},
   "source": [
    "**Ans:** BiLSTM (Bidirectional Long Short-Term Memory) is a recurrent neural network (RNN) architecture that processes input sequences in both directions with two separate hidden layers. It is a type of RNN that can remember information for long periods of time and can process both forward and backward sequences of data. It is a combination of two LSTMs, one processing the input sequence in forward direction and the other processing the sequence in backward direction. The output of both LSTMs are then combined and used as an input to a fully connected layer. BiLSTM is often used in natural language processing tasks such as sentiment analysis and language modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be98b409-74a6-4bfa-8c70-4da466880985",
   "metadata": {},
   "source": [
    "**10. Explain BiGRU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df8362-56c2-4ff6-b743-13ca425eba10",
   "metadata": {},
   "source": [
    "**Ans:** BiGRU (Bidirectional Gated Recurrent Units) is a type of recurrent neural network (RNN) architecture that processes input data in both directions, allowing the network to learn the context of a sequence of data more effectively. A BiGRU consists of two separate recurrent neural networks, one processing the input data in the forward direction and the other in the backward direction. The two networks then combine their output, allowing the network to learn the context of the entire sequence of data. This helps the network to better understand the data and is especially useful for tasks such as natural language processing and time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ea740-34b7-4740-806f-44fd4ff7dd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
