{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56da49b-1d38-4447-b0dd-e97b61644599",
   "metadata": {},
   "source": [
    "# Assignment 5 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfe814-f292-4db2-966d-d68e1ecf3b29",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8b382-62b3-4d7c-b46c-ef6e9b0fb069",
   "metadata": {},
   "source": [
    "**1. What are Sequence-to-sequence models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584fe90a-c64b-48c5-8710-9756139df8e5",
   "metadata": {},
   "source": [
    "**Ans:** Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture that is used for tasks like machine translation, text summarization, and conversation models. The architecture consists of an encoder and a decoder, which work together to transform an input sequence into an output sequence. The encoder reads in the input sequence and encodes it into a vector representation, while the decoder takes the vector representation and decodes it into the output sequence. This architecture has been used to great success in a variety of tasks and is a powerful tool for natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4fca7-7ed6-4ad7-86da-8c653583754e",
   "metadata": {},
   "source": [
    "**2. What are the Problem with Vanilla RNNs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f90d1-c721-4b8c-a9bd-79e9a5ff4611",
   "metadata": {},
   "source": [
    "**Ans:** The primary problem with vanilla RNNs is that they are prone to the vanishing gradient problem, which is a phenomenon in which the gradients of the network weights can become so small during training that they are nearly impossible to update. This is because the gradients are repeatedly multiplied together over time and can quickly become much smaller than they were initially. Additionally, vanilla RNNs struggle to remember information over long time periods due to their limited capacity for long-term memory. Lastly, vanilla RNNs are difficult to train due to their sequential nature, as they require all of the previous data points to be input into the network before the current data point can be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1439c5-d1fb-452e-84a1-b26ee75115f6",
   "metadata": {},
   "source": [
    "**3. What is Gradient clipping?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b84ef4-f166-4276-bc7a-90a0b7c899a1",
   "metadata": {},
   "source": [
    "**Ans:** Gradient clipping is a technique used to prevent the gradients in a neural network from becoming too large. It involves clipping the gradients to a predefined maximum value, which helps to prevent the gradients from exploding, which can lead to instability in training. This technique can be used to help the network converge faster and more accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea632f-d57d-4764-aecc-510a0b4321c8",
   "metadata": {},
   "source": [
    "**4. Explain Attention mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c48710-8906-449e-a925-ac997af57534",
   "metadata": {},
   "source": [
    "**Ans:** A attention mechanism is a mechanism that allows a machine learning model to focus on a specific part of a given input. It is a mechanism that allows a model to focus on the most relevant parts of the input, while ignoring irrelevant parts. Attention mechanisms are used in many applications, such as natural language processing, computer vision, and reinforcement learning. The most common form of attention is the soft attention mechanism, where the model assigns weights to different parts of the input to emphasize the most important parts. By doing so, the model can better focus on the relevant information and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede975e-9a61-4bd3-b041-edfc1aeb5f1f",
   "metadata": {},
   "source": [
    "**5. Explain Conditional random fields (CRFs)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dacbc-5218-4da4-a3b3-112eaace9298",
   "metadata": {},
   "source": [
    "**Ans:** Conditional random fields (CRFs) are a type of discriminative probabilistic model often used for labeling or parsing structured data. CRFs are a type of graphical model, meaning that they use a graph-based structure to represent the relationships between variables in the model. CRFs are used for a variety of tasks, such as part-of-speech tagging, named entity recognition, and object recognition. Unlike other probabilistic models, such as hidden Markov models or naive Bayes classifiers, CRFs are able to take into account the relationships between variables in the model. This is done by defining a probability distribution over a set of output variables that depends on a set of input variables. This allows for more accurate predictions, as the model is able to learn how different variables interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b30e5-8b9a-4eab-a17f-e0b0a44fa083",
   "metadata": {},
   "source": [
    "**6. Explain self-attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb052ad-ba1b-4db0-b53b-a43093306fa6",
   "metadata": {},
   "source": [
    "**Ans:** Self-attention is a type of neural network layer that allows for a more direct representation of relationships between input elements. It does this by computing the attention weights of each element with respect to all the other elements. This allows the model to better capture the context of the input elements and the relationships between them, leading to improved performance. Self-attention is used in a variety of models such as transformer networks, transformer-based language models, and vision transformer networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae979fa9-4a26-46c9-9686-89fe8ba0ff70",
   "metadata": {},
   "source": [
    "**7. What is Bahdanau Attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95236cae-427b-48aa-baee-31934b19a6bc",
   "metadata": {},
   "source": [
    "**Ans:** Bahdanau Attention is a type of attention mechanism developed by Dzmitry Bahdanau and Kyunghyun Cho. It is a type of Neural Machine Translation (NMT) system that uses an encoder-decoder architecture with attention. The attention mechanism works by taking a set of input vectors and computing a context vector that is used to weigh each input vector. The weighted vectors are then combined to form an output vector. The output vector is used to decode the target language sentence. The attention mechanism helps the model to focus on specific parts of the input sentence and helps to improve the accuracy of the translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fabeb7-7a41-473c-b426-52302b7c867b",
   "metadata": {},
   "source": [
    "**8. What is a Language Model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d748078-a729-4c4e-987a-30e13a36ba64",
   "metadata": {},
   "source": [
    "**Ans:** A language model is a probability distribution over sequences of words. It is a type of artificial intelligence algorithm used to predict the next word or phrase in a sequence based on the words that have already been inputted. Language models are typically used in natural language processing (NLP) applications such as machine translation, speech recognition, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e16157-6d26-45a6-bae9-39535ff9ad47",
   "metadata": {},
   "source": [
    "**9. What is Multi-Head Attention?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6d44a-b91e-45ff-9ffc-000b776566ca",
   "metadata": {},
   "source": [
    "**Ans:** Multi-Head Attention is a type of attention mechanism used in neural network architectures such as transformers. It is used to capture multiple aspects of a sequence of data by applying attention to multiple different parts of the input sequence. This allows the model to learn a more complex representation of the data and to better capture complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e91923-2f2d-464b-96e1-8d287c21ecea",
   "metadata": {},
   "source": [
    "**10. What is Bilingual Evaluation Understudy (BLEU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d64d9-c4af-4915-bd3b-6642a19b9350",
   "metadata": {},
   "source": [
    "**Ans:** BLEU (Bilingual Evaluation Understudy) is a method to evaluate the quality of machine translated texts. It was first proposed in 2002 by Kishore Papineni and colleagues as an alternative to existing methods of evaluating machine-generated translations. BLEU uses a modified version of precision to measure the quality of the translation. The score is calculated by comparing the predicted translation to a set of reference translations and computing the percentage of words in the predicted translation that also appear in the references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e243f1-265d-4d60-bd95-efa55965512b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
