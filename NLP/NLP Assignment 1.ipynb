{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f10b11-8e8d-4a78-af31-42d3d40bacc8",
   "metadata": {},
   "source": [
    "# Assignment 1 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca5ec7-2617-47e1-a3af-73f92fe37038",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fa5a1c-0260-4487-90d8-794ee2062017",
   "metadata": {},
   "source": [
    "**1. Explain One-Hot Encoding?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0782a-c70c-428c-b0dd-abf5c4808eba",
   "metadata": {},
   "source": [
    "**Ans:** One-hot encoding is a process used to convert categorical data, or data with text labels, into a numerical form that a computer can understand. It does this by creating new columns for each categorical feature and assigning a 1 or 0 (hot or cold, respectively) to each row to indicate the presence or absence of a particular feature. For example, if a dataset had a feature called \"Gender\" with three options - Male, Female, and Other - one-hot encoding would create three new columns, Male, Female, and Other, and assign a 1 or 0 to each row to indicate which option it was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c472e-444f-48ee-aaf3-a26f68f128df",
   "metadata": {},
   "source": [
    "**2. Explain Bag of Words?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0976bed-17f3-4f09-a34f-1ce73cd12c67",
   "metadata": {},
   "source": [
    "**Ans:** Bag of Words (BoW) is a technique used in natural language processing (NLP) for representing text. It is a way of extracting features from text for use in machine learning algorithms. BoW is a model representation used to simplify the often complex task of understanding natural language. It is a process of representing text as numerical feature vectors. It is one of the most common techniques used in NLP for feature extraction and is used to represent text in the form of a bag of words. The bag of words model ignores grammar and word order, but keeps track of the frequency of the words in the text. The text is represented as a numerical vector in which each word is represented by a number indicating the frequency of occurrence in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da78d9-e7f6-4625-b4d7-0547a3280475",
   "metadata": {},
   "source": [
    "**3. Explain Bag of N-Grams?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632e4d2-8458-4b40-a9a6-ec1d9003b571",
   "metadata": {},
   "source": [
    "**Ans:** Bag of N-Grams is a type of feature representation used in Natural Language Processing (NLP). It is a technique used to represent text data as numerical features, where each feature represents a collection (or “bag”) of adjacent words or “N-Grams”. N-Grams are a sequence of N words taken together, and can be a single word (Unigram), two words (Bigram), three words (Trigram), and so on. Bag of N-Grams is used in supervised learning algorithms such as text classification, sentiment analysis, and language modeling. The advantage of Bag of N-Grams is it captures the context of words in a sentence, which is especially useful when the meaning of words changes depending on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be8106-7cda-407f-9d7f-5edfee61c067",
   "metadata": {},
   "source": [
    "**4. Explain TF-IDF?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c1a428-5d3e-4339-a6b1-a308fec35111",
   "metadata": {},
   "source": [
    "**Ans:** TF-IDF (term frequency–inverse document frequency) is a statistical measure used in natural language processing (NLP) to reflect how important a word is to a document in a corpus. It is the product of two statistics, term frequency (TF) and inverse document frequency (IDF). The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. The weighting of TF-IDF is intended to represent the importance of a word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845ba98-cf17-4aee-b2da-6e8d385ff478",
   "metadata": {},
   "source": [
    "**5. What is OOV problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cd789-3649-4078-937e-88285c428d1f",
   "metadata": {},
   "source": [
    "**Ans:** OOV (Out-of-Vocabulary) is a problem in natural language processing (NLP) where a system cannot handle words that are not included in its vocabulary. OOV words are words that the system has not seen before and therefore cannot understand or process. This is a common problem when dealing with text data, as unseen words appear frequently in natural language. To address this problem, NLP systems use techniques such as word embeddings, language models, and lexicon expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b7ace7-630a-4d76-a369-c5f8197bb236",
   "metadata": {},
   "source": [
    "**6. What are word embeddings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33580d6f-f848-414f-ba5c-986c7f8bae08",
   "metadata": {},
   "source": [
    "**Ans:** Word embeddings are a type of representation for text data, where each word in the corpus is represented as a vector of real numbers. Word embeddings can capture semantic and syntactic similarities between words, allowing models to understand the meaning of words in context and accurately make predictions. They are widely used in Natural Language Processing (NLP) applications such as sentiment analysis, text classification, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64435dbb-25ea-4542-9212-f6420a8dfa53",
   "metadata": {},
   "source": [
    "**7. Explain Continuous bag of words (CBOW)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff4361-b8bb-4794-bc1f-5af26526dd8a",
   "metadata": {},
   "source": [
    "**Ans:** Continuous Bag-of-Words (CBOW) is a natural language processing model that predicts a target word from its context. The model takes as input a set of context words (also known as the “bag”) and predicts the target word that belongs in the context. CBOW is based on the idea that words that appear in the same context are likely to be related. The model uses the context words to predict the target word by taking into account the context of the words. The model is trained on a large corpus of text and learns to predict the target word from its context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f77bc18-35f7-4e49-ab9b-50b7c3d54a7f",
   "metadata": {},
   "source": [
    "**8. Explain SkipGram?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1f83b-d080-4080-b1cb-e4a7b735c86d",
   "metadata": {},
   "source": [
    "**Ans:** SkipGram is a type of neural network architecture used mainly for natural language processing (NLP). It is used to predict a target word from its surrounding context, meaning it predicts the target word given a set of words that come before and after it in the text. SkipGram works by taking in a word and its surrounding context, and then outputting a probability distribution over all possible target words that could fill in the gap. The probability distribution is then used to determine the most likely target word, given the context. SkipGram is often used in word embedding techniques, where it is used to learn meaningful dense vector representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdd61b-1195-478c-bbfa-4d808c19ad15",
   "metadata": {},
   "source": [
    "**9. Explain Glove Embeddings?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e97f3-7298-4205-a135-986b71fc98ac",
   "metadata": {},
   "source": [
    "**Ans:** Glove (Global Vectors for Word Representation) is a popular word embedding technique developed by Stanford researchers. It is a type of word vector representation that uses a neural network to learn a vector representation of words from large datasets. The resulting vectors contain semantic information about the words that is useful for many natural language processing (NLP) tasks. Glove embeddings are pre-trained on a huge corpus of text and can be used to represent any word in the corpus. The semantic information captured in the embeddings can be used to compute similarity between words, detect relationships between words, or even classify text. Glove embeddings are used in many NLP applications such as language modeling, sentiment analysis, and text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d4d82-262c-44b4-aa87-fa8071a80776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
