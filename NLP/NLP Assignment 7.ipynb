{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56da49b-1d38-4447-b0dd-e97b61644599",
   "metadata": {},
   "source": [
    "# Assignment 7 Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dfe814-f292-4db2-966d-d68e1ecf3b29",
   "metadata": {},
   "source": [
    "**SUBMITTED BY: MIHIR KUDALE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428b8bde-2605-49ba-8fca-a78ede0b8eb7",
   "metadata": {},
   "source": [
    "**1. Explain the architecture of BERT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6181d-8e27-410a-a64e-2a2440b35f02",
   "metadata": {},
   "source": [
    "**Ans:** BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model developed by Google that is used for natural language processing (NLP) tasks such as language understanding. The architecture of BERT is based on a transformer-based model, which is a type of neural network that uses attention mechanisms to process input data more efficiently than other approaches. The core component of BERT is its bidirectional encoder, which is a multi-layer transformer network that encodes text in both directions – forward and backward. BERT also uses a number of different techniques to improve its performance, including masking some of the words in the input text, training on large corpora of text, and using a special type of “pre-training” that allows the model to learn general language patterns. Finally, BERT can be fine-tuned to specific tasks by adding additional layers on top of the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd23b099-7557-4356-b34a-cdf0dd015b23",
   "metadata": {},
   "source": [
    "**2. Explain Masked Language Modeling (MLM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a4f95-aba8-48a4-b259-1c36f76f09b8",
   "metadata": {},
   "source": [
    "**Ans:** Masked Language Modeling (MLM) is a type of language modeling technique used in natural language processing (NLP) to better understand language. It involves randomly masking (hiding) a portion of the input text, then having the model predict the original word or phrase. This allows the model to better understand the context of words and phrases, as well as their relationships with each other, so it can better predict the right word or phrase in the right context. MLM can be used for a variety of tasks, such as machine translation, question answering, text classification, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698f3e5-a052-4bc1-9f6d-0e1fe336a66a",
   "metadata": {},
   "source": [
    "**3. Explain Next Sentence Prediction (NSP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373e986-10c7-470c-80f8-9ef222975e5b",
   "metadata": {},
   "source": [
    "**Ans:** Next Sentence Prediction (NSP) is a task in natural language processing (NLP) used to train language models to predict the next sentence given a previous sentence. The model is trained on large corpora of text, and is used to predict the most likely subsequent sentence given the context of the input. The goal of NSP is to enable models to better understand the context of the input, so that they can generate more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92edec-7e73-47a7-a3e8-7356ac49d2a5",
   "metadata": {},
   "source": [
    "**4. What is Matthews evaluation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f313ce36-8665-4d29-9678-3d2b0b7ab99d",
   "metadata": {},
   "source": [
    "**Ans:** Matthew's evaluation is an assessment tool used to measure the effectiveness of a particular program or intervention. It is used to evaluate the overall impact of a program on its intended target audience, including both positive and negative outcomes. It is also used to measure the effectiveness of a particular intervention against predetermined objectives. The evaluation includes an assessment of the program's impact on the target population, an assessment of the program's costs and benefits, and an evaluation of the program's effectiveness in achieving its goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89065-3412-4ff3-b766-55a45e3320bf",
   "metadata": {},
   "source": [
    "**5. What is Matthews Correlation Coefficient (MCC)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deabe95-31e3-40bf-9559-1d837f583b7c",
   "metadata": {},
   "source": [
    "**Ans:** Matthews Correlation Coefficient (MCC) is a measure of the quality of a binary classifier. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is a correlation coefficient between -1 and +1, where +1 is the perfect prediction, 0 is no better than random prediction and -1 indicates total disagreement between prediction and observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26408a0e-4f6a-4eba-a919-45e0086159ed",
   "metadata": {},
   "source": [
    "**6. Explain Semantic Role Labeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c39f6-d5ff-4bff-8d68-ca21878cf3a0",
   "metadata": {},
   "source": [
    "**Ans:** Semantic Role Labeling (SRL) is a Natural Language Processing (NLP) task that uses linguistic analysis to identify the semantic roles of each part of a sentence. It is used to identify the arguments of a sentence, as well as their semantic roles, such as agent, patient, theme, and recipient. It can also be used to answer questions such as “who did what?” or “what happened to whom?”. It is a form of shallow semantic parsing and is used to better understand the meaning of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d60e9-f70e-4b89-9e60-8d2ca0b37a1d",
   "metadata": {},
   "source": [
    "**7. Why Fine-tuning a BERT model takes less time than pretraining**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908f1f9-41fb-4cf8-a493-772423773b84",
   "metadata": {},
   "source": [
    "**Ans:** Fine-tuning a BERT model typically takes less time than pretraining because the process of fine-tuning uses the already-trained weights of the BERT model as a starting point, while pretraining requires the model to be built from scratch. This means that the fine-tuning process can be much faster because the model has already been trained on a large dataset, so it can quickly learn the task-specific parameters. Additionally, the parameters from the BERT model can be reused during fine-tuning, so there is less time spent on training and more time spent on optimizing the task-specific parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96de25c-9c39-45e1-b20e-84db2d5a8d08",
   "metadata": {},
   "source": [
    "**8. Recognizing Textual Entailment (RTE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895111a5-0e78-401e-bc5d-211b32877d03",
   "metadata": {},
   "source": [
    "**Ans:** Textual Entailment (RTE) is a task in natural language processing which aims to evaluate the degree to which a hypothesis is supported by a given text. It involves an automated system which is able to determine whether a given piece of text entails, contradicts, or is neutral with respect to a given hypothesis. In other words, it is the task of determining if a given hypothesis is true or false given a piece of text. RTE has numerous applications in areas such as sentiment analysis, summarization, question answering, and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94540cf-75ec-489a-b984-34fd14125c94",
   "metadata": {},
   "source": [
    "**9. Explain the decoder stack of GPT models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb62281-6fc3-40ee-a56d-148bd50cafb2",
   "metadata": {},
   "source": [
    "**Ans:** The decoder stack of GPT models consists of a series of layers, each of which is responsible for a different part of the language understanding process. The first layer is a token embedding layer, which takes the text tokens as input and produces a vector representation of each token. The next layer is a multi-head attention layer, which allows the model to attend to different parts of the input sequence at the same time. After the attention layer, several layers of Transformer blocks are used to further process the input sequence. These Transformer blocks, which are composed of multi-head attention and feed-forward layers, allow the model to learn relationships between the words and how they interact with each other. Finally, a fully connected layer is used to produce the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca117a-bc8e-40ae-bfdd-05cef00afc0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
